## Cache a few packs' worth of blobs in BlobReader

There's a pathological case, after forgetting some snapshots and pruning,
to keep bouncing between packs. This is SLOW because we reseek each time.

## Guard against colliding with another snapshot in the same working dir

Track working dir in some global state somewhere? Shmem?

## Better pack sizes?

Get closer to the target by estimating the ratio *since last flush*?

Check on each blob, without flushing, the size to catch gross overshoots?

Just checking without flushing isn't very effective by itself - Zstd uses 60+ MB

## Compress after the cache?

Would require a fair amount of rework, but would also be a big speedup in-cache...
Probably not worth it...

## Can we pipeline filter/unfilter?

Trouble is that breaks the nice abstraction that a filtered backend is just another backend.
It also has to be *behind* the cache, which is always unfiltered...

## Axe multiple versions of B2 if their SHAs match?

Also figure out how we're doing that in the first place.
(Failed uploads!? Seems it made it even though the upload 408'd because of the above.)

## backup accounting

Kinda weird to track new bytes in the backup threads and reused bytes in the main thread.
There's a reason for it - we don't know how many bytes trees are - but...

## Usage sizes

Have backend list give sizes so we can show the whole size of the repo?
